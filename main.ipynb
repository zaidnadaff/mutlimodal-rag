{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "import json\n",
    "\n",
    "file_path = \"./content/attention.pdf\"\n",
    "\n",
    "# Step 1: Extract all elements without chunking\n",
    "elements = partition_pdf(\n",
    "    filename=file_path,\n",
    "    strategy=\"hi_res\",\n",
    "    infer_table_structure=True,\n",
    "    extract_images_in_pdf=True,\n",
    "    extract_image_block_types=[\"Image\"],\n",
    "    extract_image_block_to_payload=True\n",
    ")\n",
    "\n",
    "# Step 2: Separate tables/images from text elements\n",
    "image_chunks = [el for el in elements if el.category in [ \"Image\"]]\n",
    "table_chunks = [el for el in elements if el.category in [\"Table\"]]\n",
    "text_elements = [el for el in elements if el.category not in [\"Table\", \"Image\"]]\n",
    "\n",
    "# Step 3: Apply chunking only to the text elements\n",
    "chunked_text = chunk_by_title(\n",
    "    text_elements,\n",
    "    max_characters=10000,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    new_after_n_chars=6000\n",
    ")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_images_base64(image_chunks):\n",
    "    images_b64 = []\n",
    "    for image_chunk in image_chunks:\n",
    "        images_b64.append(image_chunk.metadata.image_base64)\n",
    "    return images_b64\n",
    "\n",
    "b64_images = get_images_base64(image_chunks)\n",
    "# print(b64_images)\n",
    "\n",
    "# print(images[0].metadata.image_base64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import htmltabletomd\n",
    "\n",
    "\n",
    "def table_html_to_markdown(table_chunks):\n",
    "    md_tables = []\n",
    "    for table_chunk in table_chunks:\n",
    "        md_table = htmltabletomd.convert_table(table_chunk.metadata.text_as_html)\n",
    "        md_tables.append(md_table)\n",
    "    return md_tables\n",
    "\n",
    "md_tables = table_html_to_markdown(table_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\")  \n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\")  \n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
    "\n",
    "# Initialize Azure OpenAI Service client with key-based authentication    \n",
    "model = AzureChatOpenAI(  \n",
    "    azure_endpoint=endpoint,  \n",
    "    api_key=subscription_key,  \n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    model=deployment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text particularly for semantic retrieval.\n",
    "These summaries will be embedded and used to retrieve the raw text or table elements\n",
    "Give a detailed summary of the table or text below that is well optimized for retrieval.\n",
    "For any tables also add in a one line description of what the table is about besides the summary.\n",
    "Do not add additional words like Summary: etc.\n",
    "Table or text chunk:\n",
    "{element}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "summarize_chain = (\n",
    "    {\"element\": lambda x: x}  \n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()  # extracts the response as text and returns it as a string\n",
    ")\n",
    "\n",
    "try:\n",
    "    table_summaries = summarize_chain.batch(md_tables, {\"max_concurrency\": 5})\n",
    "    text_summaries = summarize_chain.batch(chunked_text,{\"max_concurrency\":5})\n",
    "except Exception as e:\n",
    "    print(f\"Batch processing error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt_template = \"\"\"Describe the image in detail. For context,\n",
    "                  the image is part of a research paper explaining the transformers\n",
    "                  architecture. Be specific about graphs, such as bar plots.\"\"\"\n",
    "messages = [\n",
    "    (\n",
    "        \"user\",\n",
    "        [\n",
    "            {\"type\": \"text\", \"text\": prompt_template},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": \"data:image/jpeg;base64,{image}\"},\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "\n",
    "image_summaries = chain.batch(b64_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def display_base64_image(base64_image):\n",
    "    base64_decode = base64.b64decode(base64_image)\n",
    "\n",
    "    display(Image(data=base64_decode))\n",
    "\n",
    "display_base64_image(b64_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\")\n",
    "model_name = os.getenv(\"EMBEDDING_DEPLOYMENT\")\n",
    "subscription_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "api_version = \"2024-12-01-preview\"\n",
    "\n",
    "\n",
    "\n",
    "openai_embed_model = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-05-01-preview\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generated successfully:\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "# Sample text to generate embeddings for\n",
    "test_text = \"Hello, world! Testing Azure OpenAI Embeddings.\"\n",
    "\n",
    "# Generate the embeddings\n",
    "try:\n",
    "    embedding = openai_embed_model.embed_query(test_text)\n",
    "    print(\"Embedding generated successfully:\")\n",
    "    # print(embedding[:10])  # Print only the first 10 values for brevity\n",
    "except Exception as e:\n",
    "    print(f\"Error generating embedding: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# Initialize in-memory document store\n",
    "memory_store = InMemoryStore()\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    docstore, text_summaries, texts, table_summaries, tables, image_summaries, \n",
    "    images\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    Assumes everything is kept in memory for local testing\n",
    "    \"\"\"\n",
    "    id_key = \"doc_id\"\n",
    "    \n",
    "    # Prepare all summaries for initial FAISS creation\n",
    "    all_summaries = []\n",
    "    all_metadata = []\n",
    "    \n",
    "    # Helper function to prepare documents\n",
    "    def prepare_documents(doc_summaries, doc_contents, doc_store):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summaries = []\n",
    "        metadata = []\n",
    "        \n",
    "        for i, s in enumerate(doc_summaries):\n",
    "            summaries.append(s)\n",
    "            metadata.append({id_key: doc_ids[i]})\n",
    "            \n",
    "        # Store the actual documents\n",
    "        doc_store.mset(list(zip(doc_ids, doc_contents)))\n",
    "        \n",
    "        return summaries, metadata\n",
    "    \n",
    "    # Prepare all document types if they exist\n",
    "    if text_summaries:\n",
    "        text_summaries_list, text_metadata = prepare_documents(text_summaries, texts, docstore)\n",
    "        all_summaries.extend(text_summaries_list)\n",
    "        all_metadata.extend(text_metadata)\n",
    "        \n",
    "    if table_summaries:\n",
    "        table_summaries_list, table_metadata = prepare_documents(table_summaries, tables, docstore)\n",
    "        all_summaries.extend(table_summaries_list)\n",
    "        all_metadata.extend(table_metadata)\n",
    "        \n",
    "    if image_summaries:\n",
    "        image_summaries_list, image_metadata = prepare_documents(image_summaries, images, docstore)\n",
    "        all_summaries.extend(image_summaries_list)\n",
    "        all_metadata.extend(image_metadata)\n",
    "    \n",
    "    # Create FAISS index from all prepared summaries\n",
    "    if all_summaries:\n",
    "        vectorstore = FAISS.from_texts(\n",
    "            texts=all_summaries,\n",
    "            embedding=openai_embed_model,\n",
    "            metadatas=all_metadata\n",
    "        )\n",
    "    else:\n",
    "        # Create empty FAISS index if no documents\n",
    "        vectorstore = FAISS.from_texts(\n",
    "            texts=[\"placeholder\"],\n",
    "            embedding=openai_embed_model,\n",
    "            metadatas=[{id_key: \"placeholder\"}]\n",
    "        )\n",
    "    \n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=docstore,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "# Create the in-memory retriever\n",
    "retriever_multi_vector = create_multi_vector_retriever(\n",
    "    memory_store,\n",
    "    text_summaries,\n",
    "    chunked_text,\n",
    "    table_summaries,\n",
    "    md_tables,\n",
    "    image_summaries,\n",
    "    b64_images, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.\"\n",
    "docs = retriever_multi_vector.invoke(query, limit=5)\n",
    "\n",
    "# We get 3 docs\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import base64\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is_image_data(docs[0])\n",
    "looks_like_base64(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from unstructured.documents.elements import CompositeElement\n",
    "\n",
    "\n",
    "retrieved_tables = []\n",
    "retrieved_images = []\n",
    "retrieved_texts = []\n",
    "\n",
    "for doc in docs:\n",
    "    if type(doc) == CompositeElement:\n",
    "        retrieved_texts.append(str(doc))\n",
    "    elif is_image_data(doc) and looks_like_base64(doc):\n",
    "        retrieved_images.append(doc)\n",
    "    else:\n",
    "        retrieved_tables.append(doc) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(retrieved_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "3.5 Positional Encoding\n",
       "\n",
       "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
       "\n",
       "5\n",
       "\n",
       "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
       "\n",
       "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and ﬁxed [8].\n",
       "\n",
       "In this work, we use sine and cosine functions of different frequencies:\n",
       "\n",
       "PE(pos,2i) = sin(pos/100002i/dmodel) PE(pos,2i+1) = cos(pos/100002i/dmodel)\n",
       "\n",
       "where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of PEpos.\n",
       "\n",
       "We also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
       "\n",
       "4 Why Self-Attention\n",
       "\n",
       "In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1,...,xn) to another sequence of equal length (z1,...,zn), with xi,zi ∈ Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\n",
       "\n",
       "One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
       "\n",
       "The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
       "\n",
       "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
       "\n",
       "6\n",
       "\n",
       "the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\n",
       "\n",
       "A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [15], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\n",
       "\n",
       "As side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2 Background\n",
       "\n",
       "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
       "\n",
       "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].\n",
       "\n",
       "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28].\n",
       "\n",
       "To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].\n",
       "\n",
       "3 Model Architecture\n",
       "\n",
       "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29]. Here, the encoder maps an input sequence of symbol representations (x1,...,xn) to a sequence of continuous representations z = (z1,...,zn). Given z, the decoder then generates an output sequence (y1,...,ym) of symbols one element at a time. At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next.\n",
       "\n",
       "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Layer Type | Complexity per Layer | Sequential Operations | Maximum Path Length |\n",
       "| :--- | :--- | :--- | :--- |\n",
       "| Self-Attention | O(n? - d) | O(1) | O(1) |\n",
       "| Recurrent | O(n-d?) | O(n) | O(n) |\n",
       "| Convolutional | O(k-n-d?) | olny | O(logx(n)) |\n",
       "| Self-Attention (restricted) | O(r-n-d) | ol) | O(n/r) |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Model | EN-DE | BLEU EN-FR | Training EN-DE | Cost (FLOPs) EN-FR |\n",
       "| :--- | :--- | :--- | :--- | :--- |\n",
       "| ByteNet 23.75 |\n",
       "| Deep-Att + PosUnk |  | 39.2 |  | 1.0 - 107° |\n",
       "| GNMT + RL Bi] | 24.6 | 39.92 | 2.3-10!9 | 1.4-1070 |\n",
       "| ConvS2S | 25.16 | 40.46 | 9.6-10'% | 1.5-1070 |\n",
       "| MoE | 26.03 | 40.56 | 2.0-10'9 | 1.2. 1079 |\n",
       "| Deep-Att + PosUnk Ensemble |  | 40.4 |  | 8.0 - 107° |\n",
       "| GNMT + RL Ensemble (BI | 26.30 | 41.16 | 1.8-1079 | 1.1- 1074 |\n",
       "| ConvS2S Ensemble [8] | 26.36 | 41.29 | 7.7-10!9 | 1.2.10?! |\n",
       "| Transformer (base model) | 27.3 | 38.1 | 3.3- | 1018 |\n",
       "| Transformer (big) | 28.4 | 41.0 | 2.3. | 1019 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for retrieved_text in retrieved_texts:\n",
    "    display(Markdown(retrieved_text))\n",
    "for retrieved_table in retrieved_tables:\n",
    "    display(Markdown(retrieved_table))\n",
    "for retrieved_image in retrieved_images:\n",
    "    display_base64_image(retrieved_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
